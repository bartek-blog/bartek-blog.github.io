{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "layout: post\n",
    "comments: true\n",
    "title:  \"Machine Learning Part 5: L1 and L2 Regularization\"\n",
    "date:   2019-03-03 08:00:00 +0200\n",
    "categories: ml python sklearn regularization\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams['figure.figsize'] = [20, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to explain very important technique called regularization. Why we need it? We have already discuss that sometimes models overfit, that is they fit themselves to train data and do not generalize well. If we observe that we could try to make model simpler. But it is not always possible and it is generally complicated to manually find out how to do this. With regularization we could try to do this in a sort of automated way. \n",
    "\n",
    "We will discuss here two very popular types of regularizations:\n",
    "* $l_2$ regularization\n",
    "* $l_1$ regularization.\n",
    "\n",
    "So are what $l_1$ and $l_2$ regularizations? The name comes from the fact that those techniques try to reduce the variance of the predictions of the model. In simple words it means that they try to make predictions smoother. They do no change hyperparameters of the model, they change the function that is use to optimize parameters. Let's see.\n",
    "\n",
    "### l2 regularization\n",
    "\n",
    "Do you remember what metric we have been using when we had evaluated regression model. We have been using MSE:\n",
    "\n",
    "$$MSE(\\hat{Y}) = \\frac{1}{N}\\sum_i^N \\left(y_i - \\hat{y}_i\\right)^2$$\n",
    "\n",
    "At the same time this metric is used when we fit the training data to the model, that is when the train algorithm finds the best parameters. We have not been discussing how exactly it is done, but it is very important to understand that in the case of Linear regression this algorithm is trying to minimize the above function. In this context it is called __loss function__ or __objective function__. \n",
    "\n",
    "It can be a bit confusing so let's repeat it and write it down. While we train the linear model\n",
    "\n",
    "$$\\hat{y} = a_1x_1 + \\ldots + a_nx_n + b$$\n",
    "\n",
    "we call the method\n",
    "```\n",
    "reg.fit(X_train, y_train).\n",
    "```\n",
    "Then the algorithm behind the `fit` tries to find best parameters $a_1, \\ldots, a_n, b$ such the the __loss function__ given by\n",
    "\n",
    "$$l(\\hat{Y}) = \\frac{1}{N}\\sum_i^N \\left(y_i - \\hat{y}_i\\right)^2$$\n",
    "\n",
    "is as small as possible. In other words it is finding global minimum.\n",
    "\n",
    "In this case __loss function__ and __evaluation metrics__ are the same. But it does not have to be that way. It is sometimes not possible to have them being equal, since optimization algorithms often require the loss function to be differentiable. But it also often beneficial to have them different. The first example is $l_1$ regularization. In this case __loss function__ is defined as follows.\n",
    "\n",
    "$$l(\\hat{Y}) = \\frac{1}{N}\\sum_i^N \\left(y_i - \\hat{y}_i\\right)^2 + \\alpha \\sum(a_1^2 + \\ldots + a_n^2)$$\n",
    "\n",
    "A linear model that use this loss function is also called __Ridge__ regression. Let's see how it works. In `sklearn` we have `Ridge` regression class. So what we will do is the following.\n",
    "1. We will prepare data by taking only numerical columns that do not have not defined values.\n",
    "2. We will do `train-dev-test` split.\n",
    "3. We will train standard linear model with it.\n",
    "4. We will train Ridge models using grid search to find the best alpha.\n",
    "5. We compare them.\n",
    "\n",
    "Later we move to $l_1$ regularization.\n",
    "\n",
    "We will explain them using an example from <kaggle.com> the competition called __House Prices: Advanced Regression Techniques__, see <https://www.kaggle.com/c/house-prices-advanced-regression-techniques>. So please download `train.cvs` dataset from there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['MSSubClass', 'MSZoning', 'LotFrontage', 'LotArea', 'Street',\n",
       "       'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig',\n",
       "       'LandSlope', 'Neighborhood', 'Condition1', 'Condition2',\n",
       "       'BldgType', 'HouseStyle', 'OverallQual', 'OverallCond',\n",
       "       'YearBuilt', 'YearRemodAdd', 'RoofStyle', 'RoofMatl',\n",
       "       'Exterior1st', 'Exterior2nd', 'MasVnrType', 'MasVnrArea',\n",
       "       'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual', 'BsmtCond',\n",
       "       'BsmtExposure', 'BsmtFinType1', 'BsmtFinSF1', 'BsmtFinType2',\n",
       "       'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'Heating', 'HeatingQC',\n",
       "       'CentralAir', 'Electrical', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF',\n",
       "       'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath',\n",
       "       'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'KitchenQual',\n",
       "       'TotRmsAbvGrd', 'Functional', 'Fireplaces', 'FireplaceQu',\n",
       "       'GarageType', 'GarageYrBlt', 'GarageFinish', 'GarageCars',\n",
       "       'GarageArea', 'GarageQual', 'GarageCond', 'PavedDrive',\n",
       "       'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch',\n",
       "       'ScreenPorch', 'PoolArea', 'PoolQC', 'Fence', 'MiscFeature',\n",
       "       'MiscVal', 'MoSold', 'YrSold', 'SaleType', 'SaleCondition',\n",
       "       'SalePrice'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "houses = pd.read_csv(\"data/house-prices-advanced-regression-techniques/train.csv\", index_col=\"Id\")\n",
    "houses.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting numerical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_numeric_columns = ['MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual', 'OverallCond',\n",
    "       'YearBuilt', 'YearRemodAdd',   'MasVnrArea', 'BsmtFinSF1', \n",
    "       'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF',\n",
    "       'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath',\n",
    "       'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', \n",
    "       'TotRmsAbvGrd',  'Fireplaces',  'GarageYrBlt', 'GarageCars',\n",
    "       'GarageArea', \n",
    "       'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch',\n",
    "       'ScreenPorch', 'PoolArea', \n",
    "       'MiscVal', 'MoSold', 'YrSold', \n",
    "       'SalePrice']\n",
    "all_non_numeric_columns= ['MSZoning', 'Street',\n",
    "       'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig',\n",
    "       'LandSlope', 'Neighborhood', 'Condition1', 'Condition2',\n",
    "       'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl',\n",
    "       'Exterior1st', 'Exterior2nd', 'MasVnrType', 'ExterQual', 'ExterCond',\n",
    "        'Foundation', 'BsmtQual', 'BsmtCond',\n",
    "       'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'Heating', 'HeatingQC',\n",
    "       'CentralAir', 'Electrical', 'KitchenQual', 'Functional', 'FireplaceQu',\n",
    "       'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'PavedDrive',\n",
    "        'PoolQC', 'Fence', 'MiscFeature',\n",
    "        'SaleType', 'SaleCondition'             ]\n",
    "numeric_columns =  ['MSSubClass', 'LotArea', 'OverallQual', 'OverallCond',\n",
    "       'YearBuilt', 'YearRemodAdd', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', \n",
    "       'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF',\n",
    "       'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath',\n",
    "       'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', \n",
    "       'TotRmsAbvGrd',  'Fireplaces', 'GarageCars', 'GarageArea', \n",
    "       'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch',\n",
    "       'ScreenPorch', 'PoolArea', 'MiscVal', 'MoSold', 'YrSold', 'SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xy = houses[numeric_columns]\n",
    "target_col = \"SalePrice\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecting columns with not defined values and removing them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_with_na = []\n",
    "for col in Xy.columns.values:\n",
    "    nas = sum(Xy[col].isna())\n",
    "    if nas > 0:\n",
    "        cols_with_na.append(col)\n",
    "        print(col, sum(Xy[col].isna()))\n",
    "        \n",
    "Xy = Xy.drop(cols_with_na, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-dev-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Xy.drop(target_col, axis=1)\n",
    "y = Xy[target_col]\n",
    "\n",
    "X_train_dev, X_test, y_train_dev, y_test = train_test_split(X, y, random_state=666, test_size=0.2)\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(X_train_dev, y_train_dev, random_state=667, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34385.234048404694\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_train, y_train)\n",
    "y_dev_hat = reg.predict(X_dev)\n",
    "print(np.sqrt(mean_squared_error(y_dev, y_dev_hat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression (l2-normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34385.21627188204 0.0009765625\n",
      "34385.198497077225 0.001953125\n",
      "34385.16295262101 0.00390625\n",
      "34385.091884315094 0.0078125\n",
      "34384.94983006951 0.015625\n",
      "34384.666050567335 0.03125\n",
      "34384.09980372219 0.0625\n",
      "34382.97252847458 0.125\n",
      "34380.73861305588 0.25\n",
      "34376.35145764797 0.5\n",
      "34367.8856120807 1\n",
      "34352.0838653716 2\n",
      "34324.302917764035 4\n",
      "34280.00538001611 8\n",
      "34218.279686735215 16\n",
      "34144.49706742589 32\n",
      "34073.99183046314 64\n",
      "34054.336869484076 128\n",
      "34206.535653918574 256\n",
      "34711.584549416664 512\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "for i in range(-10, 10):\n",
    "    alpha = 2**i\n",
    "    ridge_reg = Ridge(alpha=alpha)\n",
    "    ridge_reg.fit(X_train, y_train)\n",
    "    y_dev_hat = ridge_reg.predict(X_dev)\n",
    "    print(np.sqrt(mean_squared_error(y_dev, y_dev_hat)), alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the best model is the model with alpha 256 and it works a bit better than the standard linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regression (l1 regularization)\n",
    "\n",
    "In the case of $l_1$-regularization the __loss function__ is defined as follows.\n",
    "\n",
    "$$l(\\hat{Y}) = \\frac{1}{N}\\sum_i^N \\left(y_i - \\hat{y}_i\\right)^2 + \\alpha \\sum(|a_1| + \\ldots + |a_n|)$$\n",
    "\n",
    "The linear model with this loss function is often called Lasso. It has the same name in `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34385.232390226694 0.0009765625\n",
      "34385.23073205115 0.001953125\n",
      "34385.22741571128 0.00390625\n",
      "34385.220783122604 0.0078125\n",
      "34385.20751809857 0.015625\n",
      "34385.18098887226 0.03125\n",
      "34385.12793384828 0.0625\n",
      "34385.021837840366 0.125\n",
      "34384.80970238467 0.25\n",
      "34384.3856600976 0.5\n",
      "34383.53847847767 1\n",
      "34381.84771210018 2\n",
      "34378.480581855554 4\n",
      "34371.80402161136 8\n",
      "34358.681686191914 16\n",
      "34333.360856228275 32\n",
      "34286.429998580745 64\n",
      "34223.5487321279 128\n",
      "34173.03689578013 256\n",
      "34221.15737373851 512\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "for i in range(-10, 10):\n",
    "    alpha = 2**i\n",
    "    lasso_reg = Lasso(alpha=alpha)\n",
    "    lasso_reg.fit(X_train, y_train)\n",
    "    y_dev_hat = lasso_reg.predict(X_dev)\n",
    "    print(np.sqrt(mean_squared_error(y_dev, y_dev_hat)), alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the best parameter seems to be 128. \n",
    "\n",
    "### l1 regularization nulls the coefficients \n",
    "\n",
    "There is an interesting effect when of l1 regularization. To observe it, let's train the model with high alpha = 100000. Then let's look at coefficients. One can observe that most of them where actually nulled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-5.32584635e+01,  1.84814242e-01,  0.00000000e+00,  0.00000000e+00,\n",
       "        4.61600753e+02,  3.77706089e+02,  1.25281624e+01,  0.00000000e+00,\n",
       "       -0.00000000e+00,  2.42549189e+01,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  6.60391554e+01,  0.00000000e+00, -0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  5.98967107e+01,\n",
       "        3.91127200e+01, -0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        6.25981928e+01, -1.63844160e+02, -5.90782181e+00,  0.00000000e+00,\n",
       "       -0.00000000e+00])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso_reg = Lasso(alpha=100000)\n",
    "lasso_reg.fit(X_train, y_train)\n",
    "lasso_reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic Net: both regularization at the same time \n",
    "\n",
    "We can also have them both at the same time. In this case the __loss function__ is defined as follows.\n",
    "\n",
    "$$l(\\hat{Y}) = \\frac{1}{N}\\sum_i^N \\left(y_i - \\hat{y}_i\\right)^2 + \n",
    "\\alpha_1 \\sum(|a_1| + \\ldots + |a_n|)+\n",
    "\\alpha_1 \\sum((a_1)^2 + \\ldots + (a_n)^2)\n",
    "$$\n",
    "\n",
    "The linear model with this loss function is often called Elastic Net. \n",
    "\n",
    "It has the name `ElasticNet` in `sklearn`. It also has two parameters, but they are not $\\alpha_1$ and $\\alpha_2$. They are called `alpha` and `l1_ratio`. Then the relation between them is the following:\n",
    "\n",
    "$$\\alpha_1 = \\texttt{alpha}\\cdot\\texttt{l1_ratio}$$\n",
    "\n",
    "$$\\alpha_2 = \\texttt{alpha}\\cdot(\\texttt{1 - l1_ratio})$$\n",
    "\n",
    "The following code shows how to use `ElasticNet` class from `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "alpha_ratio_score = []\n",
    "\n",
    "for i in range(-10, 10):\n",
    "    for j in range(20):\n",
    "        alpha = 2**i\n",
    "        l1_ratio = 0.6**j\n",
    "        en_reg = ElasticNet(alpha=alpha, l1_ratio=l1_ratio)\n",
    "        en_reg.fit(X_train, y_train)\n",
    "        y_dev_hat = en_reg.predict(X_dev)\n",
    "        alpha_ratio_score.append([alpha, l1_ratio, np.sqrt(mean_squared_error(y_dev, y_dev_hat)), alpha, l1_ratio])\n",
    "        \n",
    "scores = pd.DataFrame({\n",
    "    \"alpha\": [ars[0] for ars in alpha_ratio_score],\n",
    "    \"ratio\": [ars[1] for ars in alpha_ratio_score],\n",
    "    \"score\": [ars[2] for ars in alpha_ratio_score]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alpha</th>\n",
       "      <th>ratio</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>0.125</td>\n",
       "      <td>0.027994</td>\n",
       "      <td>34049.422441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>0.125</td>\n",
       "      <td>0.016796</td>\n",
       "      <td>34049.445167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>0.125</td>\n",
       "      <td>0.046656</td>\n",
       "      <td>34049.459889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>0.125</td>\n",
       "      <td>0.010078</td>\n",
       "      <td>34049.474806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>0.125</td>\n",
       "      <td>0.006047</td>\n",
       "      <td>34049.498295</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     alpha     ratio         score\n",
       "147  0.125  0.027994  34049.422441\n",
       "148  0.125  0.016796  34049.445167\n",
       "146  0.125  0.046656  34049.459889\n",
       "149  0.125  0.010078  34049.474806\n",
       "150  0.125  0.006047  34049.498295"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.sort_values('score').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'correlated_with_target' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-f258ddea8529>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcorrelated_with_target\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcorrelated_with_target\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0my_dev_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_dev\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcorrelated_with_target\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_dev_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'correlated_with_target' is not defined"
     ]
    }
   ],
   "source": [
    "correlated_with_target.pop(7)\n",
    "reg.fit(X_train[correlated_with_target[:-1]], y_train)\n",
    "y_dev_hat = reg.predict(X_dev[correlated_with_target[:-1]])\n",
    "print(np.sqrt(mean_squared_error(y_dev, y_dev_hat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "alpha_ratio_score = []\n",
    "\n",
    "for i in range(-10, 10):\n",
    "    for j in range(20):\n",
    "        alpha = 2**i\n",
    "        l1_ratio = 0.6**j\n",
    "        en_reg = ElasticNet(alpha=alpha, l1_ratio=l1_ratio)\n",
    "        en_reg.fit(X_train[correlated_with_target[:-1]], y_train)\n",
    "        y_dev_hat = en_reg.predict(X_dev[correlated_with_target[:-1]])\n",
    "        alpha_ratio_score.append([alpha, l1_ratio, np.sqrt(mean_squared_error(y_dev, y_dev_hat)), alpha, l1_ratio])\n",
    "        \n",
    "scores = pd.DataFrame({\n",
    "    \"alpha\": [ars[0] for ars in alpha_ratio_score],\n",
    "    \"ratio\": [ars[1] for ars in alpha_ratio_score],\n",
    "    \"score\": [ars[2] for ars in alpha_ratio_score]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.sort_values(\"score\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "scaler = StandardScaler()\n",
    "#scaler = MinMaxScaler()\n",
    "reg = LinearRegression()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "reg.fit(X_train_scaled, y_train)\n",
    "y_dev_hat = reg.predict(scaler.transform(X_dev))\n",
    "print(np.sqrt(mean_squared_error(y_dev, y_dev_hat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "alpha_ratio_score = []\n",
    "\n",
    "for i in range(-10, 10):\n",
    "    for j in range(20):\n",
    "        alpha = 2**i\n",
    "        l1_ratio = 0.6**j\n",
    "        en_reg = ElasticNet(alpha=alpha, l1_ratio=l1_ratio)\n",
    "        en_reg.fit(scaler.transform(X_train), y_train)\n",
    "        y_dev_hat = en_reg.predict(scaler.transform(X_dev))\n",
    "        alpha_ratio_score.append([alpha, l1_ratio, np.sqrt(mean_squared_error(y_dev, y_dev_hat)), alpha, l1_ratio])\n",
    "        \n",
    "scores = pd.DataFrame({\n",
    "    \"alpha\": [ars[0] for ars in alpha_ratio_score],\n",
    "    \"ratio\": [ars[1] for ars in alpha_ratio_score],\n",
    "    \"score\": [ars[2] for ars in alpha_ratio_score]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.sort_values(\"score\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_params = pd.DataFrame({\n",
    "    \"column\": X_train.columns.values,\n",
    "    \"params\": reg.coef_})\n",
    "\n",
    "reg_params['params_abs'] = np.abs(reg.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_columns = reg_params.sort_values(\"params_abs\", ascending=False)[:20][\"column\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.fit(X_train[important_columns], y_train)\n",
    "y_dev_hat = reg.predict(X_dev[important_columns])\n",
    "print(np.sqrt(mean_squared_error(y_dev, y_dev_hat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
